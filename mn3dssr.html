<!-- saved from url=(0055)file:///E:/Website/charwill.github.io-master/blplc.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Efficient 3D Surface Super-resolution via Normal-based Multimodal Restoration</title></head><body>

<center><h1>Efficient 3D Surface Super-resolution via Normal-based Multimodal Restoration</h1></center>

<center><h3>Miaohui Wang <i>et al.</i> </h3></center>

<center><p>Will update soon...</p></center>


<hr><a name="Abstract"><b>Abstract</b></a>

<br>
<p>
High-fidelity 3D surface is essential for vision tasks across various domains such as medical imaging, cultural heritage preservation, quality inspection, virtual reality, and autonomous navigation. However, the intricate nature of 3D data representations poses significant challenges in restoring diverse 3D surfaces while capturing fine-grained geometric details at a low cost. This paper introduces an efficient multimodal normal-based 3D surface super-resolution (mn3DSSR) framework, designed to address the challenges of microgeometry enhancement and computational overhead. Specifically, we have constructed one of the largest photometric stereo dataset, ensuring superior data quality and diversity through meticulous subjective selection. Furthermore, we explore a new two-branch multimodal alignment approach along with a multimodal split fusion module to mitigate computational complexity while improve restoration performance. To address the limitations associated with normal-based multimodal learning, we develop novel normal-induced loss functions that facilitate geometric consistency and improve feature alignment. Extensive experiments conducted on seven benchmark datasets across four different 3D data representations demonstrate that mn3DSSR consistently outperforms state-of-the-art super-resolution methods in terms of restoration accuracy with high computational efficiency.
 </p>
 
<br>
<p>
</p><hr><a name="Download"><b>Download</b></a>
<br>
[<a href="https://docs.google.com/presentation/d/xxx">Gourd&Apple</a>]
<br>
[<a href="https://docs.google.com/presentation/d/xxx">Harvard</a>]
<br>
[<a href="https://docs.google.com/presentation/d/xxx">DiLiGenT10^2</a>]
<br>
[<a href="https://docs.google.com/presentation/d/xxx">DiLiGenT</a>]
<br>
[<a href="https://docs.google.com/presentation/d/xxx">LUCES</a>]
<br>
[<a href="https://docs.google.com/presentation/d/xxx">WPS</a>]
<br>
[<a href="https://docs.google.com/presentation/d/xxx">our WPS+</a>]
[<a href="https://docs.google.com/presentation/d/xxx">Source Code</a>] 
<br>
<p></p>
<hr>


<br>
<a name="dataset"><b>Proposed WPS+ dataset</b></a>
<p><b>A. Improvements of WPS+</b>
<br>To maintain diversity within our blPLC dataset, we have collected five high-quality or lossless video datasets widely used in the field of video compression. The basic statistical information of these datasets is provided in Table I, and their characteristics are described as follows: </p>

<p>1) We have re-captured low-quality samples from WPS to improve the overall data quality.</p>

<p>2) WPS captures RGB images through the camera sensor without Gamma correction, leading to inaccurate normal acquisition. In contrast, WPS+ has addressed this issue by applying Gamma correction before obtaining surface normal maps.</p>

<p>3) To synthesize the best normal maps, we have adopted three different PS-based methods [39], [40], and [41], which have enhanced the overall quality of 3D surface reconstruction. In contrast, WPS relies on one least square based Lambertian method [42].</p>

<p>4) We have invited three professionals to carefully evaluate and select the best 3D reconstruction results, investing over 1,000 hours to ensure high-quality samples.</p>

<p>5) To better represent diverse surface shapes, we have greatly expanded the number of dataset samples, now including 600 objects in WPS+ compared to 400 objects in WPS.</p>

<p>6) After improving data quality, we have scaled the WPS+ dataset by a larger magnification to obtain a more comprehensive super-resolution dataset, including the ×2, ×4, and ×8 sampling settings.</p>



<br>
<a name="dataset"><b>Proposed mn3DSSR method</b></a>
<p><b>A. Multimodal Pre-processing Stage (MPS)</b>
<br>MPS performs necessary feature transformation on the raw multimodal data, addressing two main issues: (1) normalizing the input to minimize variations and distribution differences across modalities from various sources, and (2) constructing primary features for the alignment module, generating shape and texture features. </p>



<p><b>B. Multimodal Swin-Transformer Alignment (MSTA)</b>
<br>To align the previously processed I′_lr and D′_lr, we propose a new two-branch multimodal Swin-Transformer alignment (MSTA) module, consisting of a RGB-texture alignment branch and a depth-shape alignment branch. Specifically, our objective is primarily to enhance the texture normal information by aligning I′_lr with N^t_lr in the RGB-texture branch. Simultaneously, we inject the global 3D geometric information into the shape normal by aligning D′_lr with N^s_lr in the depth-shape branch. </p>

<p><b>C. Multimodal Split Fusion (MSF)</b>
<br>After processing the side-modality features in the MSTA module, Ftn and Fsn are further fused into the normal modality to assist in super-resolution feature extraction. In our previous method [11], a fusion module has been developed based on a spatial feature transform to modulate side-modalities. However, this approach often neglects dynamic changes in the main normal branch and struggles to adapt to the distinct characteristics of texture and shape normal features. To address this limitation, we propose a multimodal split fusion (MSF) module that integrates texture and shape normal features.</p>




<br>
<hr>
<a name="References"><b>References</b></a>
<br>
[<i>Qian2021CVPR</i>] G. Qian, A. Abualshour, G. Li, A. Thabet, and B. Ghanem, “Pu-gcn: Point Cloud Upsampling Using Graph Convolutional Networks,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 11 683–11 692. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Feng2022CVPR</i>] W. Feng, J. Li, H. Cai, X. Luo, and J. Zhang, “Neural Points: Point Cloud Representation with Neural Fields for Arbitrary Upsampling,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 18 633–18 642. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>He2023CVPR</i>] Y. He, D. Tang, Y. Zhang, X. Xue, and Y. Fu, “Grad-PU: Arbitrary-Scale Point Cloud Upsampling via Gradient Descent with Learned Distance Functions,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 5354–5363. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Loop2008TOG</i>] C. Loop and S. Schaefer, “Approximating Catmull-Clark Subdivision Surfaces with Bicubic Patches,” ACM Transactions on Graphics, vol. 27, no. 1, pp. 1–11, 2008. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Liu2020TOG</i>] H.-T. D. Liu, V. G. Kim, S. Chaudhuri, N. Aigerman, and A. Jacobson, “Neural subdivision,” ACM Transactions on Graphics, vol. 39, no. 2, pp. 10–16, 2020. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Shim2023CVPR</i>] J. Shim, C. Kang, and K. Joo, “Diffusion-Based Signed Distance Fields for 3D Shape Generation,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 20 887–20 897. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Voynov2019ICCV</i>] O. Voynov, A. Artemov, V. Egiazarian, A. Notchenko, G. Bobrovskikh, E. Burnaev, and D. Zorin, “Perceptual Deep Depth Super-Resolution,” in IEEE International Conference on Computer Vision (ICCV), 2019, pp. 5653–5663. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Deng2021TPAMI</i>] X. Deng and P. L. Dragotti, “Deep Convolutional Neural Network for Multi-Modal Image Restoration and Fusion,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 10,pp.3333–3348, 2021. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Zhao2022CVPR</i>] Z. Zhao, J. Zhang, S. Xu, Z. Lin, and H. Pfister, “Discrete Cosine Transform Network for Guided Depth Map Super-Resolution,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 5697–5707. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Metzger2023CVPR</i>] N. Metzger, R. C. Daudt, and K. Schindler, “Guided Depth Super- Resolution by Deep Anisotropic Diffusion,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 18 237– 18 246. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Ju2022IJCV</i>] Y. Ju, B. Shi, M. Jian, L. Qi, J. Dong, and K.-M. Lam, “NormAttention-PSN: A High-frequency Region Enhanced Photometric Stereo Network with Normalized Attention,” Springer International Journal of Computer Vision, vol. 130, no. 12, pp. 3014– 3034, 2022. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Xie2022CVPR</i>] W. Xie, T. Huang, and M. Wang, “MNSRNet: Multimodal Transformer Network for 3D Surface Super-Resolution,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 12 703–12 712. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Xie2023IJCAI</i>] W. Xie, T. Huang, and M. Wang, “3D Surface Super-resolution from Enhanced 2D Normal Images: A Multimodal-driven Variational AutoEncoder Approach,” in International Joint Conference on Artificial Intelligence (IJCAI), 2023, pp. 1578–1586. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Dong2016TPAMI</i>] C. Dong, C. C. Loy, K. He, and X. Tang, “Image Super-Resolution Using Deep Convolutional Networks,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 2, pp. 295–307, 2015. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Zhang2018ECCV</i>] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, “Image Super-Resolution Using very Deep Residual Channel Attention Networks,” in European Conference on Computer Vision (ECCV), 2018, pp. 286–301. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Zhang2021TPAMI</i>] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual Dense Network for Image Restoration,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 7, pp. 2480–2495, 2021. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Chen2021CVPR</i>] W. Xie, T. Huang, and M. Wang, “3D Surface Super-resolution from Enhanced 2D Normal Images: A Multimodal-driven Variational AutoEncoder Approach,” in International Joint Conference on Artificial Intelligence (IJCAI), 2023, pp. 1578–1586. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Ma2022TPAMI</i>] C. Ma, Y. Rao, J. Lu, and J. Zhou, “Structure-Preserving Image Super-Resolution,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 11, pp. 7898–7911, 2022. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Saharia2023TPAMI</i>] C. Saharia, J. Ho,W. Chan, T. Salimans, D. J. Fleet, and M. Norouzi, “Image Super-Resolution via Iterative Refinement,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 4, pp. 4713–4726, 2023. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Zamir2023TPAMI</i>] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, M.- H. Yang, and L. Shao, “Learning Enriched Features for Fast Image Restoration and Enhancement,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 2, pp. 1934–1948, 2023. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]
<br>
[<i>Chen2023CVPR</i>] X. Chen, X. Wang, J. Zhou, Y. Qiao, and C. Dong, “Activating More Pixels in Image Super-Resolution Transformer,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 22 367–22 377. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]

<br>
[<i>Li2019CVPR</i>] Y. Li, V. Tsiminaki, R. Timofte, M. Pollefeys, and L. V. Gool, “3D appearance Super-Resolution with Deep Learning,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 9671–9680. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]<br>
[<i>Deng2021TIP</i>] X. Deng, Y. Zhang, M. Xu, S. Gu, and Y. Duan, “Deep Coupled FeedBack Network for Joint Exposure Fusion and Image Super- Resolution,” IEEE Transactions on Image Processing, vol. 30, no. 1, pp. 3098–3112, 2021. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]<br>
[<i>Georgescu2023WACV</i>] M.-I. Georgescu, R. T. Ionescu, A.-I. Miron, O. Savencu, N.- C. Ristea, N. Verga, and F. S. Khan, “Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution,” in IEEEWinter Conference on Applications of Computer Vision (WCACV), 2023, pp. 2195–2205. [ <a href="https://ieeexplore.ieee.org/abstract/document/1218189/">paper</a>, <a href="https://docs.google.com/presentation/d/xxx">implementation</a> ]







<br>



<br>
<br>
<hr align="center" size="2" width="100%">
<!-- Start of CuterCounter Code -->
<script type="text/javascript" id="clustrmaps" src="file://cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=100&amp;t=tt&amp;d=XQzdCPhOcp1cWiDF9FcCiysYiJL0pqS3ORXKLbHtc4g&amp;co=2d78ad&amp;ct=ffffff&amp;cmo=3acc3a&amp;cmn=ff5353"></script>
<!-- End of CuterCounter Code -->



 
</body></html>